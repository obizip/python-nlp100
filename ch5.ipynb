{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第 5 章: 係り受け解析\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40. 係り受け解析結果の読み込み（形態素)\n",
    "\n",
    "### 41. 係り受け解析結果の読み込み（文節・係り受け）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface: str, base: str, pos: str, pos1: str):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, morphs: list[Morph], dst: int | None, srcs: list[int]):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "\n",
    "    def to_str(self) -> str:\n",
    "        text = \"\".join([morph.surface for morph in self.morphs if morph.pos != \"記号\"])\n",
    "        return text\n",
    "\n",
    "class CabochaParser():\n",
    "    def __init__(self):\n",
    "        self.article = [] # list of sentence\n",
    "        self.sentence = [] # list of chunk\n",
    "        self.kakari_dict = defaultdict(lambda: [])\n",
    "        self.morphs = []\n",
    "        self.dst = None\n",
    "\n",
    "    def parse_line(self, line: str):\n",
    "        # 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音\n",
    "        # -> 表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）\n",
    "\n",
    "        if line.strip() == \"\":\n",
    "            return\n",
    "\n",
    "        # 係り結びを解析する処理\n",
    "        if line.strip()[0] == \"*\":\n",
    "            if len(self.morphs) > 0:\n",
    "                # 今のチャンクを処理\n",
    "                assert self.dst != None\n",
    "                chunk = Chunk(\n",
    "                    morphs=self.morphs,\n",
    "                    dst=self.dst if self.dst != -1 else None,\n",
    "                    srcs=[]\n",
    "                )\n",
    "                self.sentence.append(chunk)\n",
    "                self.morphs = []\n",
    "                self.dst = None\n",
    "\n",
    "            line = line.strip()\n",
    "            splitted = line.split(\" \")\n",
    "            src = int(splitted[1])\n",
    "            self.dst = int(splitted[2][:-1])\n",
    "            self.kakari_dict[self.dst].append(src)\n",
    "            return\n",
    "\n",
    "        if line.strip() == \"EOS\":\n",
    "            if len(self.morphs) == 0:\n",
    "                # EOSが2回続いたとき\n",
    "                return\n",
    "            # 今のチャンクを処理\n",
    "            assert self.dst != None\n",
    "            chunk = Chunk(\n",
    "                morphs=self.morphs,\n",
    "                dst=self.dst if self.dst != -1 else None,\n",
    "                srcs=[]\n",
    "            )\n",
    "            self.sentence.append(chunk)\n",
    "            self.morphs = []\n",
    "            self.dst = None\n",
    "\n",
    "            # EOSが来たら係り結びの対応を更新\n",
    "            for i in range(len(self.sentence)):\n",
    "                if self.kakari_dict[i] != []:\n",
    "                    self.sentence[i].srcs = self.kakari_dict[i]\n",
    "            self.kakari_dict = defaultdict(lambda: [])\n",
    "\n",
    "            # 今の文の処理\n",
    "            self.article.append(self.sentence)\n",
    "            self.sentence = []\n",
    "            return\n",
    "\n",
    "        # 形態素を解析する処理\n",
    "        surface, rest = line.split(\"\\t\")\n",
    "        outputs = rest.split(\",\")\n",
    "        morph = Morph(\n",
    "            surface=surface,\n",
    "            base=outputs[-3],\n",
    "            pos=outputs[0],\n",
    "            pos1=outputs[1]\n",
    "        )\n",
    "        self.morphs.append(morph)\n",
    "\n",
    "def read_cabocha(filepath: str | Path) -> list[list[Chunk]]:\n",
    "    parser = CabochaParser()\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            parser.parse_line(line)\n",
    "    return parser.article\n",
    "\n",
    "article = read_cabocha(\"./data/ai.ja.txt.parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文節  : 人工知能\n",
      "係り先: 語。文節  : （じんこうちのう、、\n",
      "係り先: 語。文節  : AI\n",
      "係り先: 〈エーアイ〉）とは、文節  : 〈エーアイ〉）とは、\n",
      "係り先: 語。文節  : 「『計算\n",
      "係り先: （）』という文節  : （）』という\n",
      "係り先: 道具を文節  : 概念と\n",
      "係り先: 道具を文節  : 『コンピュータ\n",
      "係り先: （）』という文節  : （）』という\n",
      "係り先: 道具を文節  : 道具を\n",
      "係り先: 用いて文節  : 用いて\n",
      "係り先: 研究する文節  : 『知能』を\n",
      "係り先: 研究する文節  : 研究する\n",
      "係り先: 計算機科学文節  : 計算機科学\n",
      "係り先: （）の文節  : （）の\n",
      "係り先: 一分野」を文節  : 一分野」を\n",
      "係り先: 指す文節  : 指す\n",
      "係り先: 語。文節  : 語。\n",
      "係り先: 研究分野」とも文節  : 「言語の\n",
      "係り先: 推論、文節  : 理解や\n",
      "係り先: 推論、文節  : 推論、\n",
      "係り先: 問題解決などの文節  : 問題解決などの\n",
      "係り先: 知的行動を文節  : 知的行動を\n",
      "係り先: 代わって文節  : 人間に\n",
      "係り先: 代わって文節  : 代わって\n",
      "係り先: 行わせる文節  : コンピューターに\n",
      "係り先: 行わせる文節  : 行わせる\n",
      "係り先: 技術」、または、文節  : 技術」、または、\n",
      "係り先: 研究分野」とも文節  : 「計算機\n",
      "係り先: （コンピュータ）による文節  : （コンピュータ）による\n",
      "係り先: 情報処理システムの文節  : 知的な\n",
      "係り先: 情報処理システムの文節  : 情報処理システムの\n",
      "係り先: 実現に関する文節  : 設計や\n",
      "係り先: 実現に関する文節  : 実現に関する\n",
      "係り先: 研究分野」とも文節  : 研究分野」とも\n",
      "係り先: される。"
     ]
    }
   ],
   "source": [
    "for chunk in (sentence := article[1]):\n",
    "    if chunk.dst is None:\n",
    "        continue\n",
    "    print(\"文節  : \", end=\"\")\n",
    "    for morph in chunk.morphs:\n",
    "        print(morph.surface, end=\"\")\n",
    "    print(\"\\n係り先: \", end=\"\")\n",
    "    for dst_morph in sentence[chunk.dst].morphs:\n",
    "        print(dst_morph.surface, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42. 係り元と係り先の文節の表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人工知能\t語\n",
      "じんこうちのう\t語\n",
      "AI\tエーアイとは\n",
      "エーアイとは\t語\n",
      "計算\tという\n",
      "という\t道具を\n",
      "概念と\t道具を\n",
      "コンピュータ\tという\n",
      "という\t道具を\n",
      "道具を\t用いて\n",
      "用いて\t研究する\n",
      "知能を\t研究する\n",
      "研究する\t計算機科学\n",
      "計算機科学\tの\n",
      "の\t一分野を\n",
      "一分野を\t指す\n",
      "指す\t語\n",
      "語\t研究分野とも\n",
      "言語の\t推論\n",
      "理解や\t推論\n",
      "推論\t問題解決などの\n",
      "問題解決などの\t知的行動を\n",
      "知的行動を\t代わって\n",
      "人間に\t代わって\n",
      "代わって\t行わせる\n",
      "コンピューターに\t行わせる\n",
      "行わせる\t技術または\n",
      "技術または\t研究分野とも\n",
      "計算機\tコンピュータによる\n",
      "コンピュータによる\t情報処理システムの\n",
      "知的な\t情報処理システムの\n",
      "情報処理システムの\t実現に関する\n",
      "設計や\t実現に関する\n",
      "実現に関する\t研究分野とも\n",
      "研究分野とも\tされる\n",
      "日本大百科全書(ニッポニカ)』の\t解説で\n",
      "解説で\t述べている\n",
      "情報工学者通信工学者の\t佐藤理史は\n",
      "佐藤理史は\t述べている\n",
      "次のように\t述べている\n"
     ]
    }
   ],
   "source": [
    "for sentence in article[:3]:\n",
    "    for chunk in sentence:\n",
    "        if chunk.dst is None:\n",
    "            continue\n",
    "        src = chunk.to_str()\n",
    "        dst = sentence[chunk.dst].to_str()\n",
    "        print(f\"{src}\\t{dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "道具を\t用いて\n",
      "知能を\t研究する\n",
      "一分野を\t指す\n",
      "知的行動を\t代わって\n",
      "人間に\t代わって\n",
      "コンピューターに\t行わせる\n",
      "研究分野とも\tされる\n",
      "解説で\t述べている\n",
      "佐藤理史は\t述べている\n",
      "次のように\t述べている\n"
     ]
    }
   ],
   "source": [
    "for sentence in article[:3]:\n",
    "    for chunk in sentence:\n",
    "        if chunk.dst is None:\n",
    "            continue\n",
    "        if \"名詞\" not in [morph.pos for morph in chunk.morphs]:\n",
    "            continue\n",
    "        dst_chunk = sentence[chunk.dst]\n",
    "        if \"動詞\" not in [morph.pos for morph in dst_chunk.morphs]:\n",
    "            continue\n",
    "\n",
    "        src = chunk.to_str()\n",
    "        dst = dst_chunk.to_str()\n",
    "\n",
    "        print(f\"{src}\\t{dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 44. 係り受け木の可視化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz as gv\n",
    "\n",
    "def visualize_kakariuke(sentence: list[Chunk]):\n",
    "    graph = gv.Digraph(format=\"png\", filename=\"test\")\n",
    "    for chunk in sentence:\n",
    "        if chunk.dst is None:\n",
    "            continue\n",
    "        dst_chunk = sentence[chunk.dst]\n",
    "        graph.node(chunk.to_str())\n",
    "        graph.edge(chunk.to_str(), dst_chunk.to_str())\n",
    "    graph.render()\n",
    "\n",
    "ex_article = read_cabocha(\"./data/ch5_example.parsed\")\n",
    "# ex_article = read_cabocha(\"./data/ch5-47.parsed\")\n",
    "visualize_kakariuke(ex_article[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 45. 動詞の格パターンの抽出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for sentence in article:\n",
    "    for chunk in sentence:\n",
    "        try:\n",
    "            i_verb = [morph.pos for morph in chunk.morphs].index(\"動詞\")\n",
    "            dst_morph = chunk.morphs[i_verb]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        src_morphs = []\n",
    "        for src in chunk.srcs:\n",
    "            try:\n",
    "                src_chunk = sentence[src]\n",
    "                i_joshi = [morph.pos for morph in src_chunk.morphs][::-1].index(\"助詞\")\n",
    "                src_morph = src_chunk.morphs[-(i_joshi + 1)]\n",
    "                src_morphs.append(src_morph)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if len(src_morphs) == 0:\n",
    "            continue\n",
    "\n",
    "        lines.append(f\"{dst_morph.base}\\t{' '.join(morph.base for morph in src_morphs)}\")\n",
    "with open(\"./data/output-ch5-45.txt\", \"w\") as f:\n",
    "    f.writelines(line + \"\\n\" for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = [1, 2, 3, 5, 6, 7, 8]\n",
    "# i = l[::-1].index(7)\n",
    "# # print(i)\n",
    "# l[-(i+1)]\n",
    "\n",
    "# !sort ./data/output-ch5-45.txt\n",
    "# !sort ./data/output-ch5-45.txt | grep -E \"行う|なる|与える\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 46. 動詞の格フレーム情報の抽出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用いる\tを\t道具を\n",
      "する\tて を\t用いて 知能を\n",
      "指す\tを\t一分野を\n",
      "代わる\tを に\t知的行動を 人間に\n",
      "行う\tて に\t代わって コンピューターに\n",
      "する\tも\t研究分野とも\n",
      "述べる\tで は に\t解説で 佐藤理史は 次のように\n",
      "する\tを で\t知的能力を コンピュータ上で\n",
      "する\tを\t推論判断を\n",
      "する\tを\t画像データを\n",
      "する\tて を\t解析して パターンを\n",
      "ある\tは が\t応用例は 画像認識等が\n",
      "する\tに で により\t1956年に ダートマス会議で ジョンマッカーシーにより\n",
      "用いる\tを\t記号処理を\n",
      "する\tを と\t記述を 主体と\n",
      "使う\tは でも\t現在では 意味あいでも\n",
      "呼ぶ\tも\t思考ルーチンも\n",
      "ある\tて も\t使われている ことも\n",
      "する\tを\tカウンセラーを\n",
      "出す\tが に\t人工無脳が 引き合いに\n",
      "する\tに を\t計算機に 役割を\n",
      "呼ぶ\tと\tエキスパートシステムと\n",
      "持つ\tが に\t人間が 暗黙に\n",
      "なる\tが と\t記述が 問題と\n",
      "する\tが は が\t出されるが 実現は 利用が\n",
      "知る\tは も\tアプローチとしては アプローチも\n",
      "ある\tて が は に\t困難視されている 知られているが 差は 記号的明示性に\n",
      "集める\tが を\tサポートベクターマシンが 注目を\n",
      "行う\tを に を\t経験を 元に 学習を\n",
      "ある\tも\t手法も\n",
      "する\tを に\t知性を 機械的に\n",
      "する\tにおいて\t宇宙において\n"
     ]
    }
   ],
   "source": [
    "for sentence in article[:5]:\n",
    "    for chunk in sentence:\n",
    "        try:\n",
    "            i_verb = [morph.pos for morph in chunk.morphs].index(\"動詞\")\n",
    "            dst_morph = chunk.morphs[i_verb]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        src_morphs = []\n",
    "        src_chunks: list[Chunk] = []\n",
    "        for src in chunk.srcs:\n",
    "            try:\n",
    "                src_chunk = sentence[src]\n",
    "                i_joshi = [morph.pos for morph in src_chunk.morphs][::-1].index(\"助詞\")\n",
    "                src_morph = src_chunk.morphs[-(i_joshi + 1)]\n",
    "                src_morphs.append(src_morph)\n",
    "                src_chunks.append(src_chunk)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if len(src_morphs) == 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"{dst_morph.base}\\t{' '.join(morph.base for morph in src_morphs)}\\t{' '.join(chunk.to_str() for chunk in src_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47. 機能動詞構文のマイニング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習を行う\tを に\t経験を 元に\n"
     ]
    }
   ],
   "source": [
    "for sentence in read_cabocha(\"./data/ch5-47.parsed\"):\n",
    "# for sentence in article:\n",
    "    for chunk in sentence:\n",
    "        try:\n",
    "            i_verb = [morph.pos for morph in chunk.morphs].index(\"動詞\")\n",
    "            dst_morph = chunk.morphs[i_verb]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        src_morphs = []\n",
    "        src_chunks: list[Chunk] = []\n",
    "        for src in chunk.srcs:\n",
    "            try:\n",
    "                src_chunk = sentence[src]\n",
    "                i_joshi = [morph.pos for morph in src_chunk.morphs][::-1].index(\"助詞\")\n",
    "                src_morph = src_chunk.morphs[-(i_joshi + 1)]\n",
    "                src_morphs.append(src_morph)\n",
    "                src_chunks.append(src_chunk)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if len(src_morphs) == 0:\n",
    "            continue\n",
    "\n",
    "        chunk_sahen_and_wo = None\n",
    "        i_sahen_and_wo = -1\n",
    "        for i, src_chunk in enumerate(src_chunks):\n",
    "            morphs = src_chunk.morphs\n",
    "            if len(morphs) != 2:\n",
    "                continue\n",
    "            if not(morphs[0].pos == \"名詞\" and morphs[0].pos1 == \"サ変接続\"):\n",
    "                continue\n",
    "            if not(morphs[1].pos == \"助詞\" and morphs[1].base == \"を\"):\n",
    "                continue\n",
    "            chunk_sahen_and_wo = src_chunk\n",
    "            i_sahen_and_wo = i\n",
    "\n",
    "        if chunk_sahen_and_wo is None:\n",
    "            continue\n",
    "\n",
    "        src_chunks.pop(i_sahen_and_wo)\n",
    "        src_morphs.pop(i_sahen_and_wo)\n",
    "\n",
    "        print(f\"{chunk_sahen_and_wo.to_str()}{dst_morph.base}\\t{' '.join(morph.base for morph in src_morphs)}\\t{' '.join(chunk.to_str() for chunk in src_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 48. 名詞から根へのパスの抽出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006年の -> ディープラーニング -> 登場と -> 登場により -> 行った -> なった\n",
      "ディープラーニング -> 登場と -> 登場により -> 行った -> なった\n",
      "深層学習の -> 登場と -> 登場により -> 行った -> なった\n",
      "登場と -> 登場により -> 行った -> なった\n",
      "2010年代 -> 以降の -> ビッグデータの -> 登場により -> 行った -> なった\n",
      "以降の -> ビッグデータの -> 登場により -> 行った -> なった\n",
      "ビッグデータの -> 登場により -> 行った -> なった\n",
      "登場により -> 行った -> なった\n",
      "一過性の -> 流行を -> 超えて -> 浸透して -> 行った -> なった\n",
      "流行を -> 超えて -> 浸透して -> 行った -> なった\n",
      "社会に -> 浸透して -> 行った -> なった\n",
      "浸透して -> 行った -> なった\n",
      "2016年から -> 2017年にかけて -> 導入した -> AIが -> 完全情報ゲームである -> 囲碁などの -> トップ棋士 -> プレイヤーも -> 破り -> なった\n",
      "2017年にかけて -> 導入した -> AIが -> 完全情報ゲームである -> 囲碁などの -> トップ棋士 -> プレイヤーも -> 破り -> なった\n",
      "ディープラーニングを -> 導入した -> AIが -> 完全情報ゲームである -> 囲碁などの -> トップ棋士 -> プレイヤーも -> 破り -> なった\n",
      "導入した -> AIが -> 完全情報ゲームである -> 囲碁などの -> トップ棋士 -> プレイヤーも -> 破り -> なった\n",
      "AIが -> 完全情報ゲームである -> 囲碁などの -> トップ棋士 -> プレイヤーも -> 破り -> なった\n",
      "完全情報ゲームである -> 囲碁などの -> トップ棋士 -> プレイヤーも -> 破り -> なった\n",
      "囲碁などの -> トップ棋士 -> プレイヤーも -> 破り -> なった\n",
      "トップ棋士 -> プレイヤーも -> 破り -> なった\n",
      "不完全情報ゲームである -> ポーカーの -> 世界トップクラスの -> プレイヤーも -> 破り -> なった\n",
      "ポーカーの -> 世界トップクラスの -> プレイヤーも -> 破り -> なった\n",
      "世界トップクラスの -> プレイヤーも -> 破り -> なった\n",
      "プレイヤーも -> 破り -> なった\n",
      "麻雀では -> なった\n",
      "MicrosoftSuphx(SuperPhoenix)」が -> 到達するなど -> なった\n",
      "AIとして -> 到達するなど -> なった\n",
      "十段に -> 到達するなど -> なった\n",
      "到達するなど -> なった\n",
      "時代の -> 最先端技術と -> なった\n",
      "最先端技術と -> なった\n",
      "第２次人工知能ブームでの -> 人工知能は -> 呼ばれ -> ある\n",
      "人工知能は -> 呼ばれ -> ある\n",
      "機械学習と -> 呼ばれ -> ある\n",
      "以下のような -> ものが -> ある\n",
      "ものが -> ある\n"
     ]
    }
   ],
   "source": [
    "for sentence in article[5:7]:\n",
    "    for chunk in sentence:\n",
    "        if chunk.dst is None:\n",
    "            continue\n",
    "\n",
    "        if \"名詞\" not in [morph.pos for morph in chunk.morphs]:\n",
    "            continue\n",
    "\n",
    "        cur_chunk = chunk\n",
    "        while True:\n",
    "            print(f\"{cur_chunk.to_str()}\", end=\"\")\n",
    "            if cur_chunk.dst is None:\n",
    "                break\n",
    "            print(\" -> \", end=\"\")\n",
    "            cur_chunk = sentence[cur_chunk.dst]\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 49. 名詞間の係り受けパスの抽出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
      "Xは | Yの -> 会議で | 作り出した\n",
      "Xは | Yで | 作り出した\n",
      "Xは | Yという -> 用語を | 作り出した\n",
      "Xは | Yを | 作り出した\n",
      "Xに関する -> Yの\n",
      "Xに関する -> 最初の -> Yで\n",
      "Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
      "Xの -> Yで\n",
      "Xの -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xの -> 会議で | Yを | 作り出した\n",
      "Xで | Yという -> 用語を | 作り出した\n",
      "Xで | Yを | 作り出した\n",
      "Xという -> Yを\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import math\n",
    "\n",
    "def masked_noun(chunk: Chunk, mask: str) -> str:\n",
    "    morphs = deepcopy(chunk.morphs)\n",
    "    morphs = [morph for morph in morphs if morph.pos != \"記号\"]\n",
    "    i = 0\n",
    "    # print([morph.pos for morph in morphs])\n",
    "    while i < len(morphs)-1:\n",
    "        if [morph.pos for morph in morphs[i:i+2]] == [\"名詞\", \"名詞\"]:\n",
    "            morphs.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    # print([morph.pos for morph in morphs])\n",
    "    i_noun = [morph.pos for morph in morphs].index(\"名詞\")\n",
    "    # print([morph.pos for morph in morphs])\n",
    "    text = \"\".join([morph.surface if i != i_noun else mask for i, morph in enumerate(morphs)])\n",
    "    return text\n",
    "\n",
    "for sentence in read_cabocha(\"./data/ch5_example.parsed\"):\n",
    "# for sentence in read_cabocha(\"./data/ch5-47.parsed\"):\n",
    "# for sentence in article[5:7]:\n",
    "\n",
    "    # 係り受けの対応を全て取得\n",
    "    kakariukes: list[list[int]] = []\n",
    "    for i, chunk in enumerate(sentence):\n",
    "        if chunk.dst is None:\n",
    "            continue\n",
    "\n",
    "        if \"名詞\" not in [morph.pos for morph in chunk.morphs]:\n",
    "            continue\n",
    "\n",
    "        cur_chunk = chunk\n",
    "        path = [i]\n",
    "        while True:\n",
    "            if cur_chunk.dst is None:\n",
    "                break\n",
    "            path.append(cur_chunk.dst)\n",
    "            cur_chunk = sentence[cur_chunk.dst]\n",
    "        kakariukes.append(path)\n",
    "\n",
    "    # print(kakariukes)\n",
    "    for i in range(len(kakariukes)):\n",
    "        for j in range(i + 1, len(kakariukes)):\n",
    "            kakariukeA = kakariukes[i]\n",
    "            kakariukeB = kakariukes[j]\n",
    "\n",
    "            if kakariukeB[0] in kakariukeA:\n",
    "                i_B = kakariukeA.index(kakariukeB[0])\n",
    "\n",
    "                # Xを含む文節を表示\n",
    "                i_A = 0\n",
    "                cur_chunk = sentence[kakariukeA[i_A]]\n",
    "                chunk_str = masked_noun(cur_chunk, mask=\"X\")\n",
    "                print(f\"{chunk_str}\", end=\"\")\n",
    "                i_A += 1\n",
    "\n",
    "                # XからYまでの文節を表示\n",
    "                while i_A != i_B:\n",
    "                    print(\" -> \", end=\"\")\n",
    "                    cur_chunk = sentence[kakariukeA[i_A]]\n",
    "                    print(f\"{cur_chunk.to_str()}\", end=\"\")\n",
    "                    i_A += 1\n",
    "\n",
    "                # Yを含む文節を表示\n",
    "                print(\" -> \", end=\"\")\n",
    "                cur_chunk = sentence[kakariukeA[i_B]]\n",
    "                chunk_str = masked_noun(cur_chunk, mask=\"Y\")\n",
    "                print(f\"{chunk_str}\")\n",
    "            else:\n",
    "                intersection = None\n",
    "                for i_A in range(1, len(kakariukeA)):\n",
    "                    if not kakariukeA[i_A] in kakariukeB:\n",
    "                        continue\n",
    "                    i_B = kakariukeB.index(kakariukeA[i_A])\n",
    "                    intersection = (i_A, i_B)\n",
    "                    break\n",
    "\n",
    "                if intersection is None:\n",
    "                    continue\n",
    "\n",
    "                dstA, dstB = intersection\n",
    "                i_A = 0\n",
    "                # Xを含む文節を表示\n",
    "                cur_chunk = sentence[kakariukeA[i_A]]\n",
    "                chunk_str = masked_noun(cur_chunk, mask=\"X\")\n",
    "                print(f\"{chunk_str}\", end=\"\")\n",
    "                i_A += 1\n",
    "\n",
    "                # Xから共通地点までの文節を表示\n",
    "                while i_A != dstA:\n",
    "                    print(\" -> \", end=\"\")\n",
    "                    cur_chunk = sentence[kakariukeA[i_A]]\n",
    "                    print(f\"{cur_chunk.to_str()}\", end=\"\")\n",
    "                    i_A += 1\n",
    "\n",
    "                print(\" | \", end=\"\")\n",
    "\n",
    "                i_B = 0\n",
    "                # Yを含む文節を表示\n",
    "                cur_chunk = sentence[kakariukeB[i_B]]\n",
    "                chunk_str = masked_noun(cur_chunk, mask=\"Y\")\n",
    "                print(f\"{chunk_str}\", end=\"\")\n",
    "                i_B += 1\n",
    "\n",
    "                # Yから共通地点までの文節を表示\n",
    "                while i_B != dstB:\n",
    "                    print(\" -> \", end=\"\")\n",
    "                    cur_chunk = sentence[kakariukeB[i_B]]\n",
    "                    print(f\"{cur_chunk.to_str()}\", end=\"\")\n",
    "                    i_B += 1\n",
    "\n",
    "                print(\" | \", end=\"\")\n",
    "\n",
    "                # 共通地点の文節を表示\n",
    "                cur_chunk = sentence[kakariukeB[dstB]]\n",
    "                print(f\"{cur_chunk.to_str()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
